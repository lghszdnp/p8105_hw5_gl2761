---
title: "p8105_hw5_gl2761"
author: "Gonghao Liu"
date: "11/15/2022"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(purrr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

First import all the datasets into one single dataframe

```{r}
raw_df = 
  tibble(
    files = list.files("data/"),
    path = str_c("data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

Clean the data and store them into a new dataframe called 'clean_df'

```{r}
clean_df = 
  raw_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

```{r}
clean_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

According to the result shown above, outcome values of patients in the experiment group have an obvious trend of increase, while the trend in control patients seems to continue to vibrate in a certain range. On average, patients in the experiment group have a better outcome than patients in the control group.

## Problem 2

Read in and clean data

```{r}
homicide_df =
  read.csv("./data_WP/homicide-data.csv", na = c("", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, ', ', state),
    resolved = case_when(
           disposition =="Closed without arrest" ~ "unsolved",
           disposition =="Open/No arrest" ~"unsolved",
           disposition =="Closed by arrest" ~ "solved"
         )) %>% 
  relocate(city_state)
```

Do summary by the following code

```{r}
city_summary = 
  homicide_df %>%
  group_by(city) %>%
  summarize(
    unsolved = sum(resolved == "unsolved"),
    solved = sum(resolved == "solved"),
    total = n()
    )

city_summary
```

For Baltimore, MD.
```{r}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")

baltimore_summary = 
  baltimore_df %>% 
  summarize(
    unsolved = sum(resolved == "unsolved"),
    n = n()
  )

baltimore_test = 
  prop.test(
  x = baltimore_summary %>% pull(unsolved),
  n = baltimore_summary %>% pull(n)
)

baltimore_test %>% 
  broom::tidy()
```

Another way of iteration

```{r}
city_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    unsolved = sum(resolved == "unsolved"),
    n = n()
  ) %>% 
  mutate(
    test_results = map2(unsolved, n, prop.test),
    tidy_results = map(test_results, broom::tidy)
  ) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))

city_df
```

Make a plot

```{r}
city_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

## Problem 3

Write functions to create dataset conatin mu, mu_hat, sigma_hat, and p_value for each iteration

```{r}
sim_n_sd = function(mu, n = 30, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_data %>% 
    summarize(
      mu_hat = mean(x),
      sigma_hat = sd(x),
      p_value = t.test(x, conf.level = 0.95)$p.value
    )
}

n_list = 
  list(
    "mu_0"  = 0, 
    "mu_1"  = 1, 
    "mu_2"  = 2,
    "mu_3"  = 3, 
    "mu_4"  = 4, 
    "mu_5"  = 5,
    "mu_6"  = 6)

output = vector("list", length = 7)

for (i in 1:7) {
  output[[i]] = rerun(5000, sim_n_sd(n_list[[i]])) %>% 
    bind_rows
}

sim_results_df = 
  tibble(mu = c(0, 1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, sim_n_sd(mu = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs) %>%
  mutate(rej_status = ifelse(p_value<0.05, 'reject', 'fail to reject'))
```

Make a proportion plot

```{r}
ggplot(sim_results_df,aes(x = as.factor(mu), fill = rej_status)) + 
  geom_bar(position = "fill") +
  xlab("True mu") + 
  ylab("Proportion")
```

The larger the true mean, the more possible to reject the null hypothesis

```{r}
sim_results_df %>%
  group_by(mu) %>%
  summarise_at(vars(mu_hat), list(sample_mean = mean)) %>%
    ggplot(aes(x = as.factor(mu), y = sample_mean)) + 
  geom_point() +
  xlab("True mu")
```

```{r}
sim_results_df %>%
  filter(rej_status=='reject') %>%
  group_by(mu) %>%
  summarise_at(vars(mu_hat), list(sample_mean = mean)) %>%
    ggplot(aes(x = as.factor(mu), y = sample_mean)) + 
  geom_point() +
  xlab("True mu")
```

The sample average for tests that null is rejected is not quite equal to the true value. Because we reject the null hypothesis under 95% confidence level, which means we are 95% percent sure that the sample mean is not equal to the true mean.